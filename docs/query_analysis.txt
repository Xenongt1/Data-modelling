QUESTION 1: Monthly Encounters by Specialty

SQL Query:

sql
SELECT
    DATE_FORMAT(encounter_date, '%Y-%m') as month,
    s.specialty_name,
    e.encounter_type,
    COUNT(*) as total_encounters,
    COUNT(DISTINCT e.patient_id) as unique_patients
FROM
    encounters e
    JOIN providers p ON e.provider_id = p.provider_id
    JOIN specialties s ON p.specialty_id = s.specialty_id
GROUP BY
    1,
    2,
    3
ORDER BY 1, 2 DESC;
Schema Analysis:

Tables joined: encounters, providers, specialties
Number of joins: 2
Performance:

Execution time: 0.391 seconds 
Estimated rows scanned: 30,000
Bottleneck Identified: Sorting and Grouping large datasets. While the query uses indexes effectively for joins (Nested Loop), it still has to retrieve all 30,000 encounters efficiently but then spends most of its time (133ms of 146ms)(299ms of 232ms) sorting the results for the GROUP BY operation. This CPU-bound sorting is the bottleneck.


---------------------------------------------------

QUESTION 2: Top Diagnosis-Procedure Pairs

SQL Query:

sql
SELECT
    d.icd10_code,
    p.cpt_code,
    COUNT(e.encounter_id) as Number_of_visits_with_this_pair
FROM encounters e
JOIN encounter_diagnoses ed ON e.encounter_id = ed.encounter_id
JOIN diagnoses d ON ed.diagnosis_id = d.diagnosis_id
JOIN encounter_procedures ep ON e.encounter_id = ep.encounter_id
JOIN procedures p ON ep.procedure_id = p.procedure_id
GROUP BY 1, 2
ORDER BY 3 DESC;
Schema Analysis:

Tables joined: encounters, encounter_diagnoses, diagnoses, encounter_procedures, procedures
Number of joins: 4
Performance:

Execution time: 1.624 seconds
Estimated rows scanned: 89,932
Bottleneck Identified: Row Explosion (Cartesian Product). The query joins two many-to-many junction tables (encounter_diagnoses and encounter_procedures) via the encounters table. This creates a partial Cartesian product for each encounter. For example, if an encounter has 3 diagnoses and 2 procedures, the intermediate result has 6 rows (3x2). This "fan-out" is visible in the plan where rows=45052 jumps to rows=89932. The database then has to aggregate (GROUP BY) this inflated dataset, which is expensive.
---------------------------------------------------

QUESTION 3: 30-Day Readmission Rate

SQL Query:

sql
SELECT
    s.specialty_name,
    COUNT(DISTINCT ini.encounter_id) as index_admissions,
    COUNT(DISTINCT readmit.encounter_id) as readmissions,
    (COUNT(DISTINCT readmit.encounter_id) / COUNT(DISTINCT ini.encounter_id)) * 100 as readmission_rate
FROM encounters ini
JOIN providers p ON ini.provider_id = p.provider_id
JOIN specialties s ON p.specialty_id = s.specialty_id
LEFT JOIN encounters readmit ON ini.patient_id = readmit.patient_id
    AND readmit.encounter_date > ini.discharge_date
    AND readmit.encounter_date <= DATE_ADD(ini.discharge_date, INTERVAL 30 DAY)
WHERE ini.encounter_type = 'Inpatient'
GROUP BY 1
ORDER BY 4 DESC;
Schema Analysis:

Tables joined: encounters (self-joined), providers, specialties
Number of joins: 3 (1 self-join)
Performance:

Execution time: 0.276 seconds 
Estimated rows scanned: 30,000(initial) and 6,095 (readmit) 
Bottleneck Identified: Self-Join Logic on Non-Indexed Dates.

Filtering: The query efficiently filters for 'Inpatient' encounters first, finding ~6,000 rows (from 30k).
The Nested Loop: For each of these 6,000 index admissions, it has to look up the encounters table again (readmit alias) to find matching patient IDs.
The Scan: The critical part is that the date logic (readmit.encounter_date > ...) is applied as a FILTER, not a direct index lookup range.
It scans about 4.34 rows per patient on average to check the dates.
It does this loop 6,041 times.
Impact: While not a massive volume bottleneck like Q2, the logic forces a nested loop that inherently slows down as data grows (O(N*M) complexity for the self-join).
---------------------------------------------------

QUESTION 4: Revenue by Specialty & Month

SQL Query:

sql
SELECT
    DATE_FORMAT(b.claim_date, '%Y-%m') as month,
    s.specialty_name,
    SUM(b.allowed_amount) as total_revenue
FROM billing b
JOIN encounters e ON b.encounter_id = e.encounter_id
JOIN providers p ON e.provider_id = p.provider_id
JOIN specialties s ON p.specialty_id = s.specialty_id
WHERE b.claim_status = 'Paid'
GROUP BY 1, 2
ORDER BY 1, 3 DESC;
Schema Analysis:

Tables joined: billing, encounters, providers, specialties
Number of joins: 3
Performance:

Execution time: 0.276 seconds 
Estimated rows scanned: 15,088 (Filtered Fact Table) -> 15,088 (Active Rows)
Bottleneck Identified: Multiple Joins on High Volume.

The Scan: The query starts with a Full Table Scan on billing (30,000 rows).
The Filter: It efficiently filters only for claim_status='Paid', finding roughly half the rows (~15,000).
The Hop: For each of these 15,000 rows, it must hop through 3 tables to get the grouping info:
Billing -> Encounters (to get Provider ID)
Encounters -> Providers (to get Specialty ID)
Providers -> Specialties (to get the Name)
The Cost: Even though these are fast index lookups, doing 15,000 sets of triple-jumps adds up. In a Data Warehouse, this "Star Join" would be much simpler or the data would be pre-aggregated.
